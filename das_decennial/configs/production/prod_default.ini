# This is the base production configuration file. It holds environment and setup information that applies to all configuration files.


[logging]
logfilename = DAS
loglevel = INFO
logfolder = logs

# DVS Configuration for the data vintaging system
dvs_enabled = $DVS_ENABLED
dvs_api_endpoint = https://dasexperimental.ite.ti.census.gov/api/dvs

[ENVIRONMENT]
das_framework_version = 1.1.0
grb_isv_name = Census
grb_app_name = DAS
grb_env3 = 0
grb_env4 =

[geodict]
# Geolevel_names and geolevel_lengs are implemented in inherited files, as they are different for US and for PR
#smallest to largest (no spaces)
#geolevel_names = Block,Block_Group,Tract,Tract_Group,County,State,US
#(largest geocode length to smallest, put 0 for US or US+PR (i.e. above state) level)
#geolevel_leng = 16,14,11,8,5,2,0
geocode_length = 16
geo_toplevel =
geo_bottomlevel = $GEO_BOTTOM_LEVEL
geo_path =

spine = $SPINE

# The first recommendation from the Geography Division for the choice parameter aian_areas is
aian_areas = Legal_Federally_Recognized_American_Indian_Area,
    American_Indian_Joint_Use_Area,
    Hawaiian_Home_Land,
    Alaska_Native_Village_Statistical_Area,
    State_Recognized_Legal_American_Indian_Area,
    Oklahoma_Tribal_Statistical_Area,
    Joint_Use_Oklahoma_Tribal_Statistical_Area

[setup]
setup = programs.das_setup.DASDecennialSetup
spark.loglevel = ERROR
environment = AWS_AUTO_SCALING_HOME,AWS_DEFAULT_REGION,AWS_CLOUDWATCH_HOME,AWS_ELB_HOME,AWS_PATH,BCC_HTTP_PROXY,BCC_HTTPS_PROXY,BCC_NO_PROXY,BOOTSTRAP_VERSION,CLUSTERID,DAS_ENVIRONMENT,DAS_ESB,DAS_LOGHOST,DAS_S3ROOT,FRIENDLY_NAME,GIT_SSL_NO_VERIFY,GRB_APP_NAME,GRB_ISV_NAME,GRB_LICENSE_FILE,GUROBI_HOME,JBID,MASTER,MISSION_NAME,NO_PROXY,TEMP,TMPDIR,TZ

[reader]
validate_input_data_constraints = True
readerPartitionLen = 14
reader = programs.reader.table_reader.DASDecennialReader
delimiter = \t
header = True

[engine]
;geolevel_num_part = 0,0,0,10000,4000,100,1
delete_raw = 0
save_noisy = $SAVE_NOISY
reload_noisy = 0
check_budget = off
engine = programs.engine.topdown_engine.TopdownEngine

[budget]
approx_dp_delta = 1e-10
only_dyadic_rationals = False

[gurobi]
outputflag = 1

optimalitytol = 1e-6
barconvtol = 1e-6
barqcpconvtol = 0
bariterlimit = 100000
feasibilitytol = 1e-7
presolve = -1
numericfocus = 3
method = -1
python_presolve = 1

# Control the number of threads used by Gurobi
threads = 96
# Threads for the top-geolevel
threads_root2root = 96
# Threads for each geolevel (if not top geolevel)
threads_state = 96
threads_county = 32
# Not used, commenting out
# threads_tract_group = 96
threads_tract = 16
threads_block_group = 8
threads_block = 4
l2_suboptimal_allowed = False
l2_grb_algorithm = -1
l2_grb_presolve = -1
l2_grb_presparsify = -1

[writer]
stats_dir = $DAS_S3ROOT/rpc/upload
upload_logfile = 0
save_git_commit = 1

classification_level = C_U_I//SP-CENS - Title 13 protected data
produce_flag = 1

writer = programs.writer.multi_writer.MultiWriter
write_metadata = 1
s3cat = 1
s3cat_suffix = .txt
s3cat_verbose = 0
overwrite_flag = 1
num_parts = 100
keep_attrs = geocode, syn, unit_syn, _invar, _cons

# The certificate!
# It has the logfile name followed by certificate_suffix.
certificate_suffix = .certificate.pdf
certificate_name = A very precise data set
certificate_title = Certificate of Disclosure Avoidance
certificate_person1 = Ben Bitdiddle
certificate_title1 = Novice Programmer
certificate_person2 = Alyssa P. Hacker
certificate_title2 = Supervisor

[validator]
validator = programs.stub_validator.validator

[takedown]
takedown = programs.takedown.takedown
delete_output = 0

[experiment]
run_experiment_flag = 0

[error_metrics]
error_metrics = programs.metrics.error_metrics_stub.ErrorMetricsStub

#error_metrics = programs.metrics.accuracy_metrics.AccuracyMetrics
calculate_binned_query_errors =Â False

[stats]
heartbeat_frequency = 1
# Notify when GC is run on the master node
notify_gc_master = 0

[monitoring]
# Heartbeat just tells the user and the dashboard that we are alive
print_heartbeat = False
print_heartbeat_frequency = 180
send_stacktrace = False
heartbeat_frequency = 60

# Do we log to the dashboard on successful token acquisitions and retries?
# notifying that we got a token is just for debugging; this will typically be false
notify_dashboard_gurobi_success = false

# notifying that we had to retry is a problem; typically this will be true
notify_dashboard_gurobi_retry = true

# Notifications are about the current execution of the optimizer.
# It's collected from syslog.
notification_frequency = 60

[alert]
# Print this message when the system starts up
message = Hello world!
