[DEFAULT]
INCLUDE=default.ini

[logging]
logfilename: DAS
loglevel: INFO
logfolder: logs

[ENVIRONMENT]
DAS_FRAMEWORK_VERSION: 0.0.1
GRB_ISV_NAME: Census
GRB_APP_NAME: DAS
GRB_Env3: 0
GRB_Env4:

[geodict]:
#smallest to largest (no spaces)
geolevel_names: Block,Block_Group,Tract,County,State
#(largest geocode length to smallest, put 1 for top level) (no spaces)
geolevel_leng: 16,12,11,5,2

[setup]
setup: programs.das_setup.DASDecennialSetup

# Spark config stuff
spark.name: DAS_RA
#local[6] tells spark to run locally with 6 threads
#spark.master: local[9]
#Error , only writes to log if there is an error (INFO, DEBUG, ERROR)
spark.loglevel: ERROR

[reader]
PersonData.path: s3://uscb-decennial-ite-das/title13_input_data/table9a/ri44.txt
UnitData.path: s3://uscb-decennial-ite-das/title13_input_data/table9a/ri44.txt

[engine]
#engine: programs.engine.topdown.engine
engine: programs.engine.topdown_engine.TopdownEngine

# should we delete the true data after making DP measurments (1 for True or 0 for False)
delete_raw: 0

[schema]
schema: PL94_P12

[budget]
epsilon_budget_total: 1.0


#budget in topdown order (e.g. US, State, .... , Block)
geolevel_budget_prop: 0.2,0.2,0.2,0.2,0.2



DPqueries:  hhgq, cenrace_hispanic_votingage, sex_agecat, detailed
queriesprop: 0.1125, 0.3375, 0.45,  0.1


[constraints]
#the invariants created, (no spaces)
theInvariants.Block: tot,gqhh_vect,gqhh_tot

#these are the info to build cenquery.constraint objects
theConstraints.Block: total,hhgq_total_lb,hhgq_total_ub,nurse_nva_0

minimalSchema: hhgq 

[writer]
writer: programs.writer.block_node_writer.BlockNodeWriter
write_type: nodes

# Variables Re-used by multiple writers
# Where the data gets written:
output_path: s3://uscb-decennial-ite-das/users/ashme001/temp
output_datafile_name: data

#Write the Data? 0 or 1
produce_flag: 1

#options for block_node_write 
# delete existing file (if one) 0 or 1
overwrite_flag: 1

minimize_nodes: 1
num_parts: 100

#hadoop or s3
filesystem: s3
# convert the node objects to json representation for saving

##options for mdfwriter
split_by_state: True
tables: PersonData UnitData
# only need next option if produce = True
#split_by_state: false
# only need next option if split_by_state = True
state_codes: 02 01 05 04 06 08 09 11 10 12 13 15 19 16 17 18 20 21 22 25 24 23 26 27 29 28 30 37 38 31 33 34 35 32 36 39 40 41 42 72 44 45 46 47 48 49 51 50 53 55 54 56 



[validator]
validator: programs.validator.validator
#validator: programs.stub_validator.validator
results_fname: /mnt/tmp/WNS_results

[assessment]

[takedown]
takedown: programs.takedown.takedown
delete_output: True

[experiment]
experiment: programs.experiment.experiment.experiment

run_experiment_flag: 0

[error_metrics]
error_metrics: programs.metrics.das_error_metrics.error_metrics

