[DEFAULT]
# root specifies the root location for all files; testdir specifies ???; mode specifies ???
# For the demo, the root in the current directory
root= .
testdir= .
mode= 0
INCLUDE=default.ini

[logging]
logfilename= DAS
loglevel= INFO
logfolder= logs

[ENVIRONMENT]
DAS_FRAMEWORK_VERSION= 0.0.1
GRB_ISV_NAME= Census
GRB_APP_NAME= DAS
GRB_Env3= 0
GRB_Env4=

[geodict]=
#smallest to largest (no spaces)
geolevel_names= Block,Block_Group,Tract,County,State,US+PR
#(largest geocode length to smallest, put 0 for US or US+PR (i.e. above state) level)
geolevel_leng= 16,12,11,5,2,0

[setup]
setup= programs.das_setup.DASDecennialSetup

# Spark config stuff
spark.name= DAS_TENABILITY_PERSONS_US+PR
#local[6] tells spark to run locally with 6 threads
#spark.master= local[9]
#Error , only writes to log if there is an error (INFO, DEBUG, ERROR)
spark.loglevel= ERROR

[engine]
engine= programs.engine.hdmm_errors_engine.HDMMRelativeErrorEngine
# should we delete the true data after making DP measurements (1 for True or 0 for False)
delete_raw= 0
# number of engine partitions desired (<= 0 to turn off repartitioning)
numEnginePartitions= -1

[schema]
schema= SF1_Person

[budget]
epsilon_budget_total= 1.0
# Bounded DP multiplier, aka "elementary" sensitivity used for each individual DP query
bounded_dp_multiplier= 2.0

#budget in topdown order (e.g. US, State, .... , Block)
geolevel_budget_prop= 0.2,0.2,0.15,0.15,0.15,.15

[hdmmError]
errorType= relative
errorMode= scatterplot
errorSampling= False
errorSamplingRate= -1.120982938129839128739128391283

# recommended to turn off Block & maybe Block_Group when using scatterplot mode (due to matplotlib/numpy RAM errors)
errorGeolevels= Block,Block_Group,Tract,County,State,US+PR
#errorGeolevels= Block,Block_Group,Tract,County,State,US+PR
#errorGeolevels= Tract,County,State,US+PR

# if set, hdmm errors engine will ignore global budget & instead just do all the budgets you ask it to do
#errorEpsilons= 0.1,0.5,1.0,2.0,4.0,8.0,12.0,16.0,20.0
errorEpsilons= 0.1,0.5,2.0,4.0,8.0,12.0,16.0,20.0

# s3 prefix under which we'll write tenability output / blank if we just want to collect to Master node
#tenabilityOutputRoot= s3://uscb-decennial-ite-das/users/lecle301/
tenabilityOutputRoot=

[workload]
workload= SF1_Person
workload.Block= SF1_Person
workload.Block_Group= SF1_Person
workload.Tract= SF1_Person
workload.County= SF1_Person
workload.State= SF1_Person
workload.US+PR= SF1_Person

[constraints]
#start with none
#the invariants created, (no spaces)
theInvariants.Block= tot
#theInvariants.Tract=

#these are the info to build cenquery.constraint objects
theConstraints.Block= total
#theConstraints.Tract=

#Note= you apparently need to specify this now
minimalSchema= sex

[reader]

[writer]
# Where the data gets written=
output_fname= s3://uscb-decennial-ite-das/users/lecle301/temp/
produce_flag= 0
# delete existing file (if one) 0 or 1
overwrite_flag= 0

[validator]
validator= programs.stub_validator.validator
#validator= programs.stub_validator.validator
results_fname= /mnt/tmp/lecle301_results

[assessment]

[takedown]
takedown= programs.takedown.takedown
delete_output= True

[experiment]
experiment= programs.experiment.experiment.experiment
run_experiment_flag= 0

[error_metrics]
error_metrics= programs.metrics.accuracy_metrics.AccuracyMetrics
