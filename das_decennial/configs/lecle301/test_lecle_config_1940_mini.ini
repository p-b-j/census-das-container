[DEFAULT]
# root specifies the root location for all files; testdir specifies ???; mode specifies ???
# For the demo, the root in the current directory
name= DAS2
root= .
testdir= .
mode= 0
loglevel= INFO

[LOGS]
hdfs_logfile= hdfs:///tmp/lecle_test.log
verbose= False
hdfs_log_overwrite= True

[ENVIRONMENT]
DAS_FRAMEWORK_VERSION= 0.0.1
cluster= AWS

[geodict]
#smallest to largest (no spaces)
geolevel_names= Enumdist,County,State,National
#(largest geocode length to smallest, put 1 for top level) (no spaces)
geolevel_leng= 10,6,2,1

[setup]
setup= programs.das_setup.DASDecennialSetup

# Spark config stuff
spark.name= DAS_lecle_1940
#local[6] tells spark to run locally with 6 threads
#spark.master= local[9]
#Error , only writes to log if there is an error (INFO, DEBUG, ERROR)
spark.loglevel= INFO

[reader]
# package(s).module_name.class_name of the reader module
reader= programs.reader.table_reader.DASDecennialReader
###
### List of tables (assuming 3 but might be more)
### These tables have decennial census specific process methods
### Table class methods will likely need to be rewritten for other applications
###
tables= PersonData UnitData

privacy_table= PersonData
constraint_tables= UnitData

# table_name.class= specify object type of each table
# Geography.class= table.GeographyTable implement later.
PersonData.class= programs.reader.table.DenseHistogramTable
UnitData.class= programs.reader.table.UnitFromPersonTable

# table_name.path - location of dir of filename=======
PersonData.path= s3://uscb-decennial-ite-das/1940/output_P_subset_mini.csv
UnitData.path= s3://uscb-decennial-ite-das/1940/output_H_subset_mini.csv

# file format (assuming all tables will have the same file format)
# we could change this to be "table_name.format_option" if need be.
delimiter= ,
header= True

# If a header exists, this must be set to "True".
# table_name.variables - space delimited ordered list of all the variables in a table.
# Geography.variables= state county block etc will add later.
PersonData.variables= RECTYPE YEAR DATANUM SERIAL PERNUM PERWT RELATE RELATED SEX AGE RACE RACED HISPAN HISPAND CITIZEN HISTID STATEFIP COUNTY TABTRACT ENUMDIST TABBLK GEOCODE GQTYPE
UnitData.variables= RECTYPE YEAR DATANUM SERIAL HHWT STATEFIP COUNTY GQ GQTYPE GQTYPED ENUMDIST TABTRACT TABBLK GEOCODE
linkage= GEOCODE

###
### For each variable in each table the following must be included:
### variable_name.type= "str" or "int"
### variable_name.legal= comma-delimited list of legal value expressions.
###                      an expression may be a single value or a range
###                      defined by two values with a "-" between them.
###                      All ranges are assumed to be closed on both ends.
###                      ie the range 0-115 includes both 0 and 115.
###
### For each variable the following is optional:
### variable_name.e2e=  special legal value for the 2018 end to end test.
###
### Note= we do not use table_name.variable_name.attribute to define these.
###

RECTYPE.type= str
RECTYPE.legal= H,P
YEAR.type= str
YEAR.legal= 1940
DATANUM.type= str
DATANUM.legal= 02
SERIAL.type= str
SERIAL.legal= 00000001-99999999
HHWT.type= str
HHWT.legal= 0000000100
STATEFIP.type= str
STATEFIP.legal= 00-99
COUNTY.type= str
COUNTY.legal= 0000-9999
GQ.type= int
GQ.legal= 0-9
GQTYPE.type= int
GQTYPE.legal= 0-9
GQTYPED.type= int
GQTYPED.legal= 000-999
ENUMDIST.type= str
ENUMDIST.legal= 0000-9999
GEOCODE.type= str
GEOCODE.legal= 0000000000-9999999999
TABTRACT.type= int
TABTRACT.legal= 00-9999
TABBLK.type= str
TABBLK.legal= 0-9

TABBLKGR.type= int
TABBLKGR.legal= 0-9999

PERNUM.type= str
PERNUM.legal= 0000-9999

PERWT.type=  str
PERWT.legal= 0000000100

SLWT.type= str
SLWT.legal= 0000000100

RELATE.type= str
RELATE.legal= 00-99

RELATED.type= str
RELATED.legal= 0000-9999

SEX.type= str
SEX.legal= 1-2

AGE.type= int
AGE.legal= 000-120

RACE.type= str
RACE.legal= 1-6

RACED.type= str
RACED.legal= 000-999

HISPAN.type= str
HISPAN.legal= 0-4

HISPAND.type=  str
HISPAND.legal= 000-999

CITIZEN.type= str
CITIZEN.legal= 0-4

HISTID.type= str
HISTID.legal= 0-9

###
### Some variables must be recoded before the disclosure engine runs.
### These recodes are very specific to the decennial census and in particular
### the 2018 test. However the module is pluggable or may be removed all together
### if no recodes are necessary. The following predisclosure recode guidelines
### are meant to ensure seemless integration of modules.
### (1) variables should not be recoded in place. ie don't overwrite old variables even if all the recode does is change a variables type from str to int so that it can be used as an array index for example.
### (2) recoder should operate on and return sql Row objects
# package.module_name.class_name for predisclosure recodes
PersonData.recoder= programs.reader.e2e_recoder.votingage_recoder
UnitData.recoder= programs.reader.e2e_recoder.gqunit_recoder
# table_name.recode_variables - list of new variable names
PersonData.recode_variables= VA RACE0 GQTYPE2
UnitData.recode_variables= GQTYPE2

# variable_name= space delimited list of variables needed to do the recode
VA= AGE
VA.type= int
VA.legal= 0-1

RACE0= RACE
RACE0.type= int
RACE0.legal= 0-5

GQTYPE2= GQTYPE
GQTYPE2.type= int
GQTYPE2.legal= 0-7

###
### The reader needs to know which variables to build the numpy multiarray over.
### For now the only
# rename geography -> groupby for more general use.
PersonData.geography= GEOCODE
PersonData.histogram= GQTYPE2 VA HISPAN CITIZEN RACE0

UnitData.geography= GEOCODE
UnitData.histogram= GQTYPE2
UnitData.unique= SERIAL

[engine]
engine= programs.engine.topdown_engine.TopdownEngine

# should we delete the true data after making DP measurments (1 for True or 0 for False)
delete_raw= 0

[budget]
#budget in topdown order (e.g. US+PR, State, .... , Block)
epsilon_budget_total: 1

#budget in topdown order (e.g. US, State, .... , Block)
geolevel_budget_prop: 0.25,0.25,0.25,0.25



# DP queries to create, (or None) (total budget proporiton must add to 1.0)
queriesfile: programs.engine.queries.QueriesCreator1940
DPqueries: race,racecomb, detailed
queriesprop: 0.25, 0.25,  0.5

[constraints]
#the invariants created, (no spaces)
theInvariants= tot,va,gqhh_vect,gqhh_tot
invariants= programs.reader.invariants.InvariantsCreator1940

#these are the info to build cenquery.constraint objects
theConstraints= total,voting_age,hhgq_total_lb,hhgq_total_ub,hhgq_va_lb,hhgq_va_ub
constraints= programs.reader.constraints.ConstraintsCreator1940


[gurobi]
OutputFlag= 0
OptimalityTol= 1e-9
BarConvTol= 1e-8
BarQCPConvTol= 0
BarIterLimit= 1000
FeasibilityTol= 1e-9
Threads= 1
Presolve= -1
NumericFocus= 3
# buildType is default, hybrid, or quicksum
buildType= hybrid

[writer]
#writer= programs.stub_writer.writer
#writer= programs.mdfwriter.MDFWriter
writer= programs.block_node_writer.BlockNodeWriter

# Variables Re-used by multiple writers
# Where the data gets written:
output_fname= s3://uscb-decennial-ite-das/users/lecle301/test1
#output_fname= /mnt/tmp/ashme001/test1

#Write the Data? 0 or 1
produce_flag= 0

# brett json arg
write_type= off

#options for block_node_write
# delete existing file (if one) 0 or 1
overwrite_flag= 1

#pickle l2geoimp nat_to_nat inputs to core node?
pickle_l2geoimp= True

#hadoop or s3
filesystem= s3

##options for mdfwriter
split_by_state= True
tables= PersonData UnitData
# only need next option if produce = True
#split_by_state= false
# only need next option if split_by_state = True
state_codes= 02 01 05 04 06 08 09 11 10 12 13 15 19 16 17 18 20 21 22 25 24 23 26 27 29 28 30 37 38 31 33 34 35 32 36 39 40 41 42 72 44 45 46 47 48 49 51 50 53 55 54 56



[validator]
validator= programs.validator.validator
#validator= programs.stub_validator.validator
results_fname= /mnt/tmp/PL_results

[assessment]

[takedown]
takedown= programs.takedown.takedown
delete_output= True

[experiment]
run_experiment_flag= 0

[error_metrics]
error_metrics= programs.metrics.das_error_metrics.error_metrics
