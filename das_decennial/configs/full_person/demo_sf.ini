[default]
include=../default.ini

[constraints]
theinvariants.block = gqhh_vect, gqhh_tot
theinvariants.state = tot
theconstraints.block = hhgq_total_lb, hhgq_total_ub, hhgq1_lessthan15, hhgq2_greaterthan25, hhgq3_lessthan20, hhgq5_lt16gt65, hhgq6_lt17gt65
theconstraints.state = total, hhgq_total_lb, hhgq_total_ub
failsafe_schema = hhgq

[validator]
validator = programs.stub_validator.validator
results_fname = /mnt/tmp/lecle_results

[error_metrics]
error_metrics = programs.metrics.error_metrics_stub.ErrorMetricsStub

[writer]
writer = programs.writer.pickled_block_data_writer.PickledBlockDataWriter
keep_attrs = geocode, syn
produce_flag = 1
overwrite_flag = 1
num_parts = 30000
stats_dir = $DAS_S3ROOT/rpc/upload
include = Writer/default.ini
output_path = s3://uscb-decennial-ite-das/users/$JBID/DemoSF/full_person/
output_datafile_name = persons
write_metadata = 0
s3cat = 0
s3cat_suffix = .txt
s3cat_verbose = 1

[takedown]
takedown = programs.takedown.takedown
delete_output = 0

[geodict]
geolevel_names = Block,Block_Group,Tract,Tract_Group,County,State,US+PR
geolevel_leng = 16,12,11,9,5,2,0
# Name of the level we want to start from (instead of root), set empty for root
geo_toplevel: US+PR
# Name of the level we want to end at (instead of leaf), set empty for leaf
geo_bottomlevel: Block

[checkpoint]
checkpoint_postfix: checkpoint
checkpoint_ignore_read: false
checkpoint_ignore_write: false

[engine]
engine = programs.engine.topdown_sfengine.TopdownSFEngine
delete_raw = 0
save_noisy = 0
reload_noisy = 0
check_budget = off

[logging]
logfilename = DAS
loglevel = INFO
logfolder = logs

[reader]
comment = ?
reader = programs.reader.table_reader.DASDecennialReader
tables = Person Unit
privacy_table = Person
constraint_tables = Unit
person.class = programs.reader.sql_spar_table.SQLSparseHistogramTable
unit.class = programs.reader.sql_spar_table.SQLSparseHistogramTable
person.path = s3://uscb-decennial-ite-das/title13_input_data/table1a_20190709/hi15.txt
unit.path = s3://uscb-decennial-ite-das/title13_input_data/table10/hi15.txt
#person.path = s3://uscb-decennial-ite-das/title13_input_data/table1a_20190709/
#unit.path = s3://uscb-decennial-ite-das/title13_input_data/table10/
delimiter = \t
header = True
person.variables = MAFID age geocode white black aian asian nhopi other hispanic sex citizen relation
unit.variables = MAFID geocode gqtype
linkage = geocode
geocode.type = str
geocode.legal = 0000000000000000-9999999999999999
mafid.type = str
mafid.legal = 000000000-999999999
sex.type = int
sex.legal = 0,1
age.type = int
age.legal = 0-115
hispanic.type = int
hispanic.legal = 0,1
white.type = int
white.legal = 0,1
black.type = int
black.legal = 0,1
aian.type = int
aian.legal = 0,1
asian.type = int
asian.legal = 0,1
nhopi.type = int
nhopi.legal = 0,1
other.type = int
other.legal = 0,1
citizen.type = int
citizen.legal = 0,1
relation.type = int
relation.legal = 0-42
ten.type = int
ten.legal = 0-3
gqtype.type = str
gqtype.legal = 000-999
vacs.type = int
vacs.legal = 0-7
person.recoder = programs.reader.e2e_recoder.DHCP_HHGQ_recoder
person.recode_variables = cenrace hhgq
cenrace = white black aian asian nhopi other
hhgq = relation
cenrace.type = int
cenrace.legal = 0-62
hhgq.type = int
hhgq.legal = 0-7
unit.recoder = programs.reader.hh_recoder.Table10RecoderSimple
unit.recode_variables = hhgqinv
hhgqinv = gqtype
hhgqinv.type = int
hhgqinv.legal = 0-7
person.geography = geocode
person.histogram = hhgq sex age hispanic cenrace citizen
numreaderpartitions = 5000
readerpartitionlen = 12
unit.geography = geocode
unit.histogram = hhgqinv
include = Reader/unit_simple.ini
validate_input_data_constraints = False

[setup]
setup = programs.das_setup.DASDecennialSetup
spark.name = DAS_DEMO_PRODUCT_DHCP
spark.loglevel = ERROR

[assessment]


[gurobi]
outputflag = 1
optimalitytol = 1e-9
barconvtol = 1e-8
barqcpconvtol = 0
bariterlimit = 1000
feasibilitytol = 1e-9
threads = 60
presolve = -1
numericfocus = 3
python_presolve = 1

[budget]
epsilon_budget_total = 4.0
# listed in same order as listed in geodict section
geolevel_budget_prop =  0.12, 0.12, 0.12, 0.12, 0.12, 0.2, 0.2


[hdmm]
#pidentity or marginal or none_strategy
strategy_type: none_strategy
#strategy_type: marginal
#ps_parameters for pidentity strategy type only (needs to be same length as hist_shape)
#ps_parameters: 1,1,2,1

[workload]
workload: demo_products_workload
workload_weight_name: demo_products_weights

[schema]
schema = DHCP_HHGQ

[ENVIRONMENT]
das_framework_version = 0.0.1
grb_isv_name = Census
grb_app_name = DAS
grb_env3 = 0
grb_env4 =
