# LAST CONFIRMED FUNCTIONAL: 3/19/2020 (Philip Leclerc)
[DEFAULT]
include = ../default.ini
epsilon = 100.
run = 1
opt_approach = DataIndUserSpecifiedQueriesNPass
run_state = ri
run_fips = 44

[ENVIRONMENT]

[python]

[gurobi]
gurobi_logfile_name= /mnt/tmp/${JBID}gurobi.log
stats_partitions= 50
l2_optimization_approach= %(opt_approach)s

[logging]
logfilename= DAS
loglevel= INFO
logfolder= logs

[geodict]
#smallest to largest (no spaces)
geolevel_names= Block,Block_Group,Tract,Tract_Group,County,State
#(largest geocode length to smallest, put 1 for top level) (no spaces)
geolevel_leng= 16,14,11,8,5,2

[setup]
setup= programs.das_setup.DASDecennialSetup
# Spark config stuff
spark.name= DAS_VA_DHCP_HHGQ_NNLS_vs_OLS_Experiment_%(opt_approach)s
#Error , only writes to log if there is an error (INFO, DEBUG, ERROR)
spark.loglevel= ERROR

[reader]
INCLUDE=Reader/unit_simple.ini
#Person.path= s3://uscb-decennial-ite-das/title13_input_data/table1a_20190709/va51.txt
#Unit.path= s3://uscb-decennial-ite-das/title13_input_data/table10/va51.txt
Person.path= s3://uscb-decennial-ite-das/title13_input_data/table1a_20190709/ri44.txt
Unit.path= s3://uscb-decennial-ite-das/title13_input_data/table10/ri44.txt
numReaderPartitions= 500
readerPartitionLen= 14
validate_input_data_constraints= False
partition_by_block_group= off

[engine]
engine= programs.engine.topdown_engine.TopdownEngine
check_budget= off
delete_raw= 0
#geolevel_num_part= 0,0,300,5,1
#saved_noisy_app_id= application_1584132156517_0023
postprocess_only= off
#pool_measurements= on
reload_noisy= off
save_noisy= off
noisy_measurements_postfix= noisy_measurements-eps%(epsilon)s-run%(run)s

[schema]
schema= DHCP_HHGQ

[budget]
epsilon_budget_total= %(epsilon)s
geolevel_budget_prop= 0.2, 0.2, 0.15, 0.15, 0.15, 0.15
dpqueries= total, hhgq, votingage * hispanic * cenrace, age * sex * hispanic * cenrace, detailed
queriesprop= 0.3, 0.15, 0.2, 0.25,  0.1

L2_DPqueryPart0= total, hhgq
L2_DPqueryPart1= votingage * hispanic * cenrace
L2_DPqueryPart2= age * sex * hispanic * cenrace
L2_DPqueryPart3= detailed

#DPqueryEstimate0= total
#DPqueryOLSUse0= total, hhgq, votingage * hispanic * cenrace, age * sex * hispanic * cenrace, detailed
#DPqueryEstimate1= hhgq, votingage * hispanic * cenrace
#DPqueryOLSUse1= hhgq, votingage * hispanic * cenrace, age * sex * hispanic * cenrace, detailed
#DPqueryEstimate2= age * sex * hispanic * cenrace
#DPqueryOLSUse2= age * sex * hispanic * cenrace, detailed
#DPqueryEstimate3= detailed
#DPqueryOLSUse3= detailed

[constraints]
#the invariants created, (no spaces)
theInvariants.Block= gqhh_vect, gqhh_tot
theInvariants.State= tot
#these are the info to build cenquery.constraint objects
theConstraints.Block= hhgq_total_lb, hhgq_total_ub, nurse_nva_0
theConstraints.State= total, hhgq_total_lb, hhgq_total_ub
minimalSchema= hhgq

[writer]
writer= programs.writer.multi_writer.MultiWriter
#multiwriter_writers= BlockNodeDicts
multiwriter_writers= BlockNodeDicts, DHCP_MDF
#writer= programs.writer.block_node_writer.BlockNodeWriter

output_path= $DAS_S3ROOT/users/${JBID}/cnstatDdpSchema_%(opt_approach)s_%(run_state)s_dpQueries100_unitCoeffs2_largeEps/
output_datafile_name= data
produce_flag= 1
keep_attrs= geocode, syn, raw, raw_housing
# delete existing file (if one) 0 or 1
overwrite_flag= 1
save_git_commit= 1
# combine output into a single file
s3cat= 1
s3cat_suffix= .txt
s3cat_verbose= 1
num_parts= 0

[validator]
#validator= programs.validator.validator
validator= programs.stub_validator.validator
results_fname= /mnt/tmp/${JBID}_results

[assessment]

[takedown]
takedown= programs.takedown.takedown
delete_output= False

[experiment]
experiment= programs.experiment.config_loops_exp.ConfigLoopsExperimentEngineWriter
run_experiment_flag= 1
loop1= FOR DEFAULT.run = 1 TO 1
loop2= FOR DEFAULT.epsilon IN 100.0

[error_metrics]
error_metrics= programs.metrics.accuracy_metrics.AccuracyMetrics
