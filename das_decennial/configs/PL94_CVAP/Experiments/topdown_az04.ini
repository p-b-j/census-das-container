[DEFAULT]
INCLUDE=../default.ini

[spark]
# Whatever spark options you may have

[logging]
logfilename: DAS
loglevel: INFO
logfolder: logs

[ENVIRONMENT]
DAS_FRAMEWORK_VERSION: 0.0.1
GRB_ISV_NAME: Census
GRB_APP_NAME: DAS
GRB_Env3: 0
GRB_Env4:

[geodict]:
#smallest to largest (no spaces)
geolevel_names: Block,Block_Group,Tract,County,State
#(largest geocode length to smallest, put 0 for US or US+PR (i.e. above state) level)
geolevel_leng: 16,12,11,5,2

[setup]
setup: programs.das_setup.DASDecennialSetup

# Spark config stuff
spark.name: DAS_AZ_TEST
#local[6] tells spark to run locally with 6 threads
#spark.master: local[9]
#Error , only writes to log if there is an error (INFO, DEBUG, ERROR)
spark.loglevel: ERROR

[reader]
# doesn't include pr72.txt since pr72.txt has null values for the citizen variable
PersonData.path: s3://uscb-decennial-ite-das/title13_input_data/table9a/az04.txt
UnitData.path: s3://uscb-decennial-ite-das/title13_input_data/table9a/az04.txt

;reader: programs.reader.pickled_blocks_syn2raw_reader.PickledBlockSyn2RawReader
;pickled.path: s3://uscb-decennial-ite-das/users/zhura301/temp/data
PersonData.class: programs.reader.sql_spar_table.SQLSparseHistogramTable
UnitData.class: programs.reader.spar_table.UnitFromPersonRepartitioned
;numReaderPartitions: 50
numReaderPartitions: 500
;measure_rdd_times: on
validate_input_data_constraints: False

[engine]
engine: programs.engine.topdown_engine.TopdownEngine
;saved_noisy_app_id: application_1548266612000_0544
;postprocess_only: on
check_budget: off
;pool_measurements: on
reload_noisy: on


# should we delete the true data after making DP measurments (1 for True or 0 for False)
delete_raw: 0

[schema]
schema: PL94_CVAP

[budget]
epsilon_budget_total: 1.0


#budget in topdown order (e.g. US, State, .... , Block)
geolevel_budget_prop: 0.2,0.2,0.2,0.2,0.2



DPqueries: hhgq, cenrace * hispanic * votingage * citizen, detailed
queriesprop: 0.225, 0.675,  0.1

[workload]
workload: PL94_CVAP

[constraints]
#the invariants created, (no spaces)
theInvariants.Block: gqhh_vect, gqhh_tot
theInvariants.Tract: tot

#these are the info to build cenquery.constraint objects
theConstraints.Block: hhgq_total_lb, hhgq_total_ub, nurse_nva_0
theConstraints.Tract: total, hhgq_total_lb, hhgq_total_ub

minimalSchema: hhgq



[writer]
writer: programs.writer.pickled_block_data_writer.PickledBlockDataWriter

keep_attrs: geocode, raw, syn

# Where the data gets written:
# ignored/overwritten when experiments are on
output_path: s3://uscb-decennial-ite-das/users/moran331/temp/
output_datafile_name: data

produce_flag: 1
#keep_attrs: geocode, syn, raw_housing

# delete existing file (if one) 0 or 1
overwrite_flag: 0
save_git_commit: 1

[validator]
validator: programs.stub_validator.validator
#validator: programs.stub_validator.validator
results_fname: /mnt/tmp/RA_results

[assessment]

[takedown]
takedown: programs.takedown.takedown
delete_output: False

[experiment]
experiment: programs.experiment.experiment.experiment

run_experiment_flag: 1

experiment_saveloc: s3://uscb-decennial-ite-das/users/moran331/experiments/PL94_CVAP/ManualTopdown_az04_25runs_multiplePLBs/test/

# we want to save the = q0,q1original data separate from the privatized data; this allows us to do so
# the original data saveloc only works if the save original data flag is on (1)
save_original_data_flag: 0
original_data_saveloc: s3://uscb-decennial-ite-das/experiments/original_data

# when this is turned on (1), the s3 terminal commands to recursively remove the RDD folders
# will be invoked in order to clear it out before the saveAsPickleFile function gets called
overwrite_flag: 1

filesystem: s3

budget_groups: td001, td01, td025, td05, td1, td2, td3, td5, td10

num_runs: 25

# Budgets follow the order of the geolevels listed in the geodict section
# e.g. Block, Block_Group, Tract, County, State, US

td001.epsilon_budget_total: 0.01
td001.geolevel_budget_prop: 0.2,0.2,0.2,0.2,0.2
td001.DPqueries: hhgq, cenrace * hispanic * votingage * citizen,detailed
td001.queriesprop: 0.225, 0.675,0.1

td01.epsilon_budget_total: 0.1
td01.geolevel_budget_prop: 0.2,0.2,0.2,0.2,0.2
td01.DPqueries: hhgq, cenrace * hispanic * votingage * citizen,detailed
td01.queriesprop: 0.225, 0.675,0.1

td025.epsilon_budget_total: 0.25
td025.geolevel_budget_prop: 0.2,0.2,0.2,0.2,0.2
td025.DPqueries: hhgq, cenrace * hispanic * votingage * citizen,detailed
td025.queriesprop: 0.225, 0.675,0.1

td05.epsilon_budget_total: 0.5
td05.geolevel_budget_prop: 0.2,0.2,0.2,0.2,0.2
td05.DPqueries: hhgq, cenrace * hispanic * votingage * citizen,detailed
td05.queriesprop: 0.225, 0.675,0.1

td1.epsilon_budget_total: 1.0
td1.geolevel_budget_prop: 0.2,0.2,0.2,0.2,0.2
td1.DPqueries: hhgq, cenrace * hispanic * votingage * citizen,detailed
td1.queriesprop: 0.225, 0.675,0.1

td2.epsilon_budget_total: 2.0
td2.geolevel_budget_prop: 0.2,0.2,0.2,0.2,0.2
td2.DPqueries: hhgq, cenrace * hispanic * votingage * citizen,detailed
td2.queriesprop: 0.225, 0.675,0.1

td3.epsilon_budget_total: 3.0
td3.geolevel_budget_prop: 0.2,0.2,0.2,0.2,0.2
td3.DPqueries: hhgq, cenrace * hispanic * votingage * citizen,detailed
td3.queriesprop: 0.225, 0.675,0.1

td5.epsilon_budget_total: 5.0
td5.geolevel_budget_prop: 0.2,0.2,0.2,0.2,0.2
td5.DPqueries: hhgq, cenrace * hispanic * votingage * citizen,detailed
td5.queriesprop: 0.225, 0.675,0.1

td10.epsilon_budget_total: 10.0
td10.geolevel_budget_prop: 0.2,0.2,0.2,0.2,0.2
td10.DPqueries: hhgq, cenrace * hispanic * votingage * citizen,detailed
td10.queriesprop: 0.225, 0.675,0.1


[error_metrics]
error_metrics: programs.metrics.accuracy_metrics_workload.AccuracyMetricsWorkload

[gurobi]
