[DEFAULT]
INCLUDE=../default.ini

[spark]
# Whatever spark options you may have

[logging]
logfilename: DAS
loglevel: INFO
logfolder: logs

[ENVIRONMENT]
DAS_FRAMEWORK_VERSION: 0.0.1
GRB_ISV_NAME: Census
GRB_APP_NAME: DAS
GRB_Env3: 0
GRB_Env4:

[geodict]:
### Which geolevels do we want to target?
# We might want to include Tract_Group as an additional level, in which we'd use the following:
# includes Tract_Group in-between Tract and County
# geolevel_names: Block,Block_Group,Tract,Tract_Group,County,State,US+PR
# includes 9 in-between 11 and 5
# geolevel_leng: 16,12,11,9,5,2,0

# Otherwise, use the following, which targets the Census Geolattice "Central Path" geolevels
# smallest to largest (no spaces)
geolevel_names: Block,Block_Group,Tract,County,State,US+PR
# (largest geocode length to smallest, put 0 for US+PR level) (no spaces)
geolevel_leng: 16,12,11,5,2,1

[setup]
setup: programs.das_setup.DASDecennialSetup

# Spark config stuff
spark.name: PL94 CVAP Official US+PR Experiment
#local[6] tells spark to run locally with 6 threads
#spark.master: local[9]
#Error , only writes to log if there is an error (INFO, DEBUG, ERROR)
spark.loglevel: ERROR

[reader]
# Which input data are we going to use for this experiment?
# We want one that includes the variables that support the PL94_CVAP schema/histogram:
# hhgq, votingage, hispanic, cenrace, citizen
PersonData.path: s3://uscb-decennial-ite-das/title13_input_data/table9a/
UnitData.path: s3://uscb-decennial-ite-das/title13_input_data/table9a/

;reader: programs.reader.pickled_blocks_syn2raw_reader.PickledBlockSyn2RawReader
;pickled.path: s3://uscb-decennial-ite-das/users/zhura301/temp/data
PersonData.class: programs.reader.sql_spar_table.SQLSparseHistogramTable
UnitData.class: programs.reader.spar_table.UnitFromPersonRepartitioned
;numReaderPartitions: 50
numReaderPartitions: 500
;measure_rdd_times: on
validate_input_data_constraints: False

[engine]
engine: programs.engine.topdown_engine.TopdownEngine
;saved_noisy_app_id: application_1548266612000_0544
;postprocess_only: on
# turn check_budget off as we don't want to spend additional computation time during an experiment to check that the budget has been allocated appropriately
check_budget: off
;pool_measurements: on
# Pavel's new changes should make it so that the saved and reloaded noisy measurements are saved in the run_i directory of the experiment, so turning them both on shouldn't be an issue. However, to save on computation time during an experiment, keeping them off is recommended. If, instead, the noisy measurements are to be analyzed later, turning save_noisy on might be a good idea, particularly if they aren't overwritten/deleted after being saved.
save_noisy: off
reload_noisy: off


# should we delete the true data after making DP measurments (1 for True or 0 for False)
delete_raw: 0

[schema]
schema: PL94_CVAP

[budget]
epsilon_budget_total: 1.0


#budget in topdown order (e.g. US, State, .... , Block)
# This indicates the PLB to be used for this data product (not the total PLB for all data products... in this way, this PLB is a "local" PLB)
geolevel_budget_prop: 0.2, 0.2, 0.15, 0.15, 0.15, 0.15

[workload]
# This specifies the workload (i.e. list of queries that we want to target to build tables and other products)
workload: PL94_CVAP

[constraints]
#the invariants created, (no spaces)
# These are the invariants we want to use
# They should be:
#   gqhh_vect: at the Block level -- refers to number of each type of group quarters and household in an Block
#   gqhh_tot:  at the Block level -- refers to the total number of group quarters and households in a Block
#   tot:       at the State level -- refers to the total population in a State
theInvariants.Block: gqhh_vect, gqhh_tot
theInvariants.State: tot

#these are the info to build cenquery.constraint objects
theConstraints.Block: hhgq_total_lb, hhgq_total_ub, nurse_nva_0
theConstraints.State: total, hhgq_total_lb, hhgq_total_ub

minimalSchema: hhgq



[writer]
writer: programs.writer.pickled_block_data_writer.PickledBlockDataWriter

# The Block-level RDD that gets written out by the PickledBlockDataWriter contains elements that are dictionaries
# keep_attrs defines which attributes of the GeounitNode class are retained/stored in these dictionary elements of the RDD
# By default, we retain the "geocode", "raw", and "syn" attributes, which correspond to the BLOCKID, CEF data, and Privatized DAS output data, respectively.
keep_attrs: geocode, raw, syn

# Where the data gets written:
# ignored/overwritten when experiments are turned on...
# Note that experiments are toggled on when the run_experiment_flag is set to 1 in the [experiment] section
output_path: s3://uscb-decennial-ite-das/users/moran331/temp/
output_datafile_name: data

# if this is turned off, then the writer will not write the pickled RDD to s3
produce_flag: 1


# delete existing file (if one) 0 or 1
overwrite_flag: 0


[validator]
validator: programs.stub_validator.validator
#validator: programs.stub_validator.validator
results_fname: /mnt/tmp/RA_results

[assessment]

[takedown]
takedown: programs.takedown.takedown
delete_output: False

[experiment]
# the experiment class is specifically for HDMM
experiment: programs.experiment.experiment_hdmm.ExperimentHDMM

# use 1 to tell the system to run an experiment with the settings in this section
# use 0 to tell the system to ignore the experiment settings and run the DAS as normal
run_experiment_flag: 1

# This is the location in s3 where all of the experiment files and data will be stored
experiment_saveloc: s3://uscb-decennial-ite-das/experiments/PL94_CVAP/HDMM_Official_National_Experiment/


# we want to save the original data separate from the privatized data; this allows us to do so
# the original data saveloc only works if the save original data flag is on (1)
save_original_data_flag: 0
original_data_saveloc: s3://uscb-decennial-ite-das/experiments/original_data

# when this is turned on (1), the s3 terminal commands to recursively remove the RDD folders
# will be invoked in order to clear it out before the saveAsPickleFile function gets called
overwrite_flag: 1

filesystem: s3


# names given to the different algorithm settings that we want to run in the experiment
budget_groups: td01, td025, td05, td1, td2, td4, td8, td16, td32, td64, td100

# each budget_group will be run this many times.
num_runs: 25

# Budgets follow the order of the geolevels listed in the geodict section
# e.g. Block, Block_Group, Tract, County, State, US

td01.epsilon_budget_total: 0.1
td01.geolevel_budget_prop: 0.2, 0.2, 0.15, 0.15, 0.15, 0.15
td01.workload: PL94_CVAP

td025.epsilon_budget_total: 0.25
td025.geolevel_budget_prop: 0.2, 0.2, 0.15, 0.15, 0.15, 0.15
td025.workload: PL94_CVAP

td05.epsilon_budget_total: 0.5
td05.geolevel_budget_prop: 0.2, 0.2, 0.15, 0.15, 0.15, 0.15
td05.workload: PL94_CVAP

td1.epsilon_budget_total: 1.0
td1.geolevel_budget_prop: 0.2, 0.2, 0.15, 0.15, 0.15, 0.15
td1.workload: PL94_CVAP

td2.epsilon_budget_total: 2.0
td2.geolevel_budget_prop: 0.2, 0.2, 0.15, 0.15, 0.15, 0.15
td2.workload: PL94_CVAP

td4.epsilon_budget_total: 4.0
td4.geolevel_budget_prop: 0.2, 0.2, 0.15, 0.15, 0.15, 0.15
td4.workload: PL94_CVAP

td8.epsilon_budget_total: 8.0
td8.geolevel_budget_prop: 0.2, 0.2, 0.15, 0.15, 0.15, 0.15
td8.workload: PL94_CVAP

td16.epsilon_budget_total: 16.0
td16.geolevel_budget_prop: 0.2, 0.2, 0.15, 0.15, 0.15, 0.15
td16.workload: PL94_CVAP

td32.epsilon_budget_total: 32.0
td32.geolevel_budget_prop: 0.2, 0.2, 0.15, 0.15, 0.15, 0.15
td32.workload: PL94_CVAP

td64.epsilon_budget_total: 64.0
td64.geolevel_budget_prop: 0.2, 0.2, 0.15, 0.15, 0.15, 0.15
td64.workload: PL94_CVAP

td100.epsilon_budget_total: 100.0
td100.geolevel_budget_prop: 0.2, 0.2, 0.15, 0.15, 0.15, 0.15
td100.workload: PL94_CVAP



[error_metrics]
error_metrics: programs.metrics.accuracy_metrics_workload.AccuracyMetricsWorkload

[gurobi]
