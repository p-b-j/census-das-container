[DEFAULT]
# root specifies the root location for all files; testdir specifies ???; mode specifies ???
# For the demo, the root in the current directory
root: .
testdir: .
mode: 0
INCLUDE=../das_decennial/configs/default.ini

[spark]
# Whatever spark options you may have

[logging]
logfilename: DAS
loglevel: INFO
logfolder: logs

[ENVIRONMENT]
DAS_FRAMEWORK_VERSION: 0.0.1
GRB_ISV_NAME: Standalone
cluster: EDU
GRB_APP_NAME: DAS
GRB_Env3: 0
GRB_Env4:

[geodict]:
geocode_length = 16
geolevel_leng = 16,12,11,5,2,0
geolevel_names = Block,Block_Group,Tract,County,State,US

[setup]
setup: programs.das_setup.DASDecennialSetup

# Spark config stuff
spark.name: DAS_CEF_TEST
#local[6] tells spark to run locally with 6 threads
#spark.master: local[9]
#Error , only writes to log if there is an error (INFO, DEBUG, )
spark.loglevel: Error

[reader]
reader = programs.reader.table_reader.DASDecennialReader
delimiter = |
header = False
tables = Unit Person
constraint_tables = Unit
privacy_table = Person
num_parts = 5
numReaderPartitions: 1

grfc_path = $HOME/das_files/seattle_arboretum.grfc
Person.path = $HOME/das_files/seattle_arboretum_per.cef
Unit.path = $HOME/das_files/seattle_arboretum_unit.cef

Person.class = programs.reader.cef_2020.cef_2020_dhcp_reader.CEF2020PersonsTable
Unit.class = programs.reader.cef_2020.cef_2020_dhcp_reader.CEF2020DHCPUnitTable

Person.generated_module = programs.reader.cef_2020.cef_validator_classes
Person.generated_table = CEF20_PER

Unit.generated_module = programs.reader.cef_2020.cef_validator_classes
Unit.generated_table = CEF20_UNIT

Person.histogram = hhgq votingage hispanic cenrace_das

Person.recoder = programs.reader.cef_2020.cef_2020_dhcp_reader.PL94_2020_recoder
Person.recode_variables = hhgq votingage hispanic cenrace_das

# variable_name= space delimited list of variables needed to do the recode
hhgq = qgqtyp
hhgq.type = int
hhgq.legal = 0-7

votingage = qage
votingage.type = int
votingage.legal = 0-1

hispanic = cenhisp
hispanic.type = int
hispanic.legal = 0-1

cenrace_das: cenrace
cenrace_das.type: int
cenrace_das.legal: 0-62

Unit.histogram = hhgq_unit_simple_recoded

Unit.recoder =  programs.reader.cef_2020.cef_2020_dhcp_reader.PL94_2020_Unit_recoder
Unit.recode_variables = hhgq_unit_simple_recoded

hhgq_unit_simple_recoded = qgqtyp
hhgq_unit_simple_recoded.type = int
hhgq_unit_simple_recoded.legal = 0-7

Person.geography = geocode
Unit.geography = geocode

[engine]
engine: programs.engine.topdown_engine.TopdownEngine
check_budget: off
;pool_measurements: on

# should we delete the true data after making DP measurments (1 for True or 0 for False)
delete_raw: 0
geolevel_num_part: 1,1,1,1,1,1

[schema]
schema: PL94_2020_SCHEMA

[budget]
strategy: strategy1a
query_ordering: redistricting_regular_ordering_1a
#budget in topdown order (e.g. US, State, .... , Block)
epsilon_budget_total = 1.0

#budget in topdown order (e.g. US, State, .... , Block)
geolevel_budget_prop = 0.2, 0.2, 0.2, 0.2, 0.1, 0.1

global_scale: 1/1

[workload]
workload: old_pl94_manual_workload

[constraints]
minimalschema = hhgq
theconstraints.block = hhgq_total_lb, hhgq_total_ub, nurse_nva_0
theconstraints.state = hhgq_total_lb, hhgq_total_ub
theconstraints.us = total
theinvariants.block = gqhh_vect, gqhh_tot
theinvariants.state = tot

[writer]
certificate_name = A very precise data set
certificate_person1 = Ben Bitdiddle
certificate_person2 = Alyssa P. Hacker
certificate_suffix = .certificate.pdf
certificate_title = Certificate of Disclosure Avoidance
certificate_title1 = Novice Programmer
certificate_title2 = Supervisor
classification_level = C_U_I//SP-CENS - Title 13 protected data
keep_attrs = geocode, syn, unit_syn, _invar, _cons, raw, raw_housing
num_parts = 5
numReaderPartitions: 1
output_datafile_name = MDF10_PER_US

stats_dir: $HOME/das_files/upload
output_path: $HOME/das_files/results

upload_logfile = 0
writer= programs.writer.multi_writer.MultiWriter
multiwriter_writers= MDFPersonAny
write_metadata = 1
overwrite_flag = 1
produce_flag = 1
s3cat = 0
s3cat_suffix = .txt
s3cat_verbose = 0
save_git_commit = 1

[validator]
validator: programs.stub_validator.validator
#validator: programs.stub_validator.validator
results_fname: $HOME/das_files/valid_results

[assessment]

[takedown]
takedown: programs.takedown.takedown
delete_output: False

[experiment]
run_experiment_flag: 0

[error_metrics]
error_metrics: programs.metrics.accuracy_metrics.AccuracyMetrics

[gurobi]
gurobi_path: /usr/local/gurobi911/linux64/${PYTHON_VERSION}_utf32/
gurobi_logfile_name: gurobi.log
gurobi_lic: ${HOME}/gurobi.lic
record_gurobi_stats: False
record_cpu_stats: False
print_gurobi_stats: True
gurobi_lic_create: False
