[DEFAULT]
# root specifies the root location for all files; testdir specifies ???; mode specifies ???
# For the demo, the root in the current directory
root: .
testdir: .
mode: 0
INCLUDE=../das_decennial/configs/default.ini

[logging]
logfilename: DAS
loglevel: INFO
logfolder: logs

[ENVIRONMENT]
DAS_FRAMEWORK_VERSION: 0.0.1
GRB_ISV_NAME: Standalone
cluster: EDU
GRB_APP_NAME: DAS
GRB_Env3: 0
GRB_Env4:

[geodict]:
aian_areas = Legal_Federally_Recognized_American_Indian_Area, American_Indian_Joint_Use_Area, Hawaiian_Home_Land, Alaska_Native_Village_Statistical_Area, State_Recognized_Legal_American_Indian_Area, Oklahoma_Tribal_Statistical_Area, Joint_Use_Oklahoma_Tribal_Statistical_Area
geocode_length = 16
geolevel_leng = 16,12,11,5,2,0
geolevel_names = Block,Block_Group,Tract,County,State,US
geo_bottomlevel = Block
geo_path = 
geo_toplevel = 
ignore_gqs_in_block_groups = False
spine = opt_spine

[setup]
setup: programs.das_setup.DASDecennialSetup

spark.name: DAS_CEF_TEST
#Error , only writes to log if there is an error (INFO, DEBUG, )
spark.loglevel: Error

[reader]
cenrace_das = cenrace
cenrace_das.legal = 0-62
cenrace_das.type = int
constraint_tables = Unit
delimiter = |
grfc_path = $HOME/das_files/grfc_combined.csv
header = False
hhgq = qgqtyp
hhgq.legal = 0-7
hhgq.type = int
hhgq_unit_simple_recoded = qgqtyp
hhgq_unit_simple_recoded.legal = 0-7
hhgq_unit_simple_recoded.type = int
hispanic = cenhisp
hispanic.legal = 0-1
hispanic.type = int
numreaderpartitions = 7
person.class = programs.reader.cef_2020.cef_2020_dhcp_reader.CEF2020PersonsTable
person.generated_module = programs.reader.cef_2020.cef_validator_classes
person.generated_table = CEF20_PER
person.geography = geocode
person.histogram = hhgq votingage hispanic cenrace_das
person.path = $HOME/das_files/converted_synth_pop.cef
person.recoder = programs.reader.cef_2020.cef_2020_dhcp_reader.PL94_2020_recoder
person.recode_variables = hhgq votingage hispanic cenrace_das
privacy_table = Person
reader = programs.reader.table_reader.DASDecennialReader
readerpartitionlen = 14
tables = Unit Person
unit.class = programs.reader.cef_2020.cef_2020_dhcp_reader.CEF2020DHCPUnitTable
unit.generated_module = programs.reader.cef_2020.cef_validator_classes
unit.generated_table = CEF20_UNIT
unit.geography = geocode
unit.histogram = hhgq_unit_simple_recoded
unit.path = $HOME/das_files/converted_synth_unit.cef
unit.recoder = programs.reader.cef_2020.cef_2020_dhcp_reader.PL94_2020_Unit_recoder
unit.recode_variables = hhgq_unit_simple_recoded
validate_input_data_constraints = True
votingage = qage
votingage.legal = 0-1
votingage.type = int

[engine]
check_budget = off
delete_raw = 0
engine = programs.engine.topdown_engine.TopdownEngine
noisy_measurements_postfix = NMF10_PER_US
reload_noisy = 0
save_noisy = TRUE

[schema]
schema: PL94_2020_SCHEMA

[budget]
approx_dp_delta = 1e-10
dp_mechanism = discrete_gaussian_mechanism
geolevel_budget_prop = 51/1024,153/1024,78/1024,51/1024,172/1024,519/1024
global_scale = 429/439
only_dyadic_rationals = False
privacy_framework = zcdp
query_ordering = Strategy1b_ST_CTY_BG_isoTot_Ordering
strategy = Strategy1b_St_Cty_BG_optSpine_ppmfCandidate

; [workload]
; workload: old_pl94_manual_workload

[constraints]
minimalschema = hhgq
theconstraints.block = hhgq_total_lb, hhgq_total_ub, nurse_nva_0
theconstraints.state = hhgq_total_lb, hhgq_total_ub
theconstraints.us = total
theinvariants.block = gqhh_vect, gqhh_tot
theinvariants.state = tot

[writer]
certificate_name = A test configuration of the DAS
certificate_person1 = Porter Jones
#certificate_person2 = Alyssa P. Hacker
certificate_suffix = .certificate.pdf
certificate_title = Certificate of Disclosure Avoidance
certificate_title1 = Grad Student
#certificate_title2 = Supervisor
classification_level = C_U_I//SP-CENS - Title 13 protected data
keep_attrs = geocode, syn, unit_syn, _invar, _cons, raw, raw_housing
num_parts = 200
numReaderPartitions: 200
output_datafile_name = MDF10_PER_US

stats_dir: $HOME/das_files/upload
output_path: $HOME/das_files/results

upload_logfile = 0
writer= programs.writer.multi_writer.MultiWriter
multiwriter_writers= MDFPersonAny
write_metadata = 1
overwrite_flag = 1
produce_flag = 1
s3cat = 1
s3cat_suffix = .txt
s3cat_verbose = 0
save_git_commit = 1

[validator]
validator: programs.stub_validator.validator
results_fname: $HOME/das_files/valid_results

[assessment]

[takedown]
takedown: programs.takedown.takedown
delete_output: False

[experiment]
run_experiment_flag: 0

[error_metrics]
; calculate_binned_query_errors = False
; calculate_per_query_quantile_errors = False
; calculate_per_query_quantile_signed_errors = False
; print_block_and_county_total_pop_errors = False
; print_blau_quintile_errors = False
; print_aians_l1_error_on_total_pop = False
; print_place_mcd_ose_bg_l1_error_on_total_pop = False
; print_8_cell_cenrace_hisp_errors = False
error_metrics = programs.metrics.accuracy_metrics.AbstractDASErrorMetrics
; l1_relative_error_geolevels = Place, Block_Group, OSE
; l1_relative_error_queries = cenrace_7lev_two_comb * hispanic, gqlevels
; population_cutoff = 500

[gurobi]
gurobi_path: /usr/local/gurobi911/linux64/${PYTHON_VERSION}_utf32/
gurobi_logfile_name: gurobi.log
gurobi_lic: ${HOME}/gurobi.lic
record_gurobi_stats: False
record_cpu_stats: False
print_gurobi_stats: True
gurobi_lic_create: False
