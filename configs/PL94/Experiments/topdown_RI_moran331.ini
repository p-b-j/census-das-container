[DEFAULT]
include=../default.ini


[logging]
logfilename: DAS
loglevel: INFO
logfolder: logs

[ENVIRONMENT]
DAS_FRAMEWORK_VERSION: 0.0.1
GRB_ISV_NAME: Census
GRB_APP_NAME: DAS
GRB_Env3: 0
GRB_Env4:

[geodict]:
#smallest to largest (no spaces)
geolevel_names: Block,Block_Group,Tract,County,State
#(largest geocode length to smallest, put 0 for US or US+PR (i.e. above state) level)
geolevel_leng: 16,12,11,5,2

[setup]
setup: programs.das_setup.DASDecennialSetup

# Spark config stuff
spark.name: DAS_RI_TEST
#local[6] tells spark to run locally with 6 threads
#spark.master: local[9]
#Error , only writes to log if there is an error (INFO, DEBUG, ERROR)
spark.loglevel: ERROR

[reader]
INCLUDE=../default.ini

PersonData.path: s3://uscb-decennial-ite-das/title13_input_data/table8/ri44.txt
UnitData.path: s3://uscb-decennial-ite-das/title13_input_data/table8/ri44.txt

[engine]
#engine: programs.engine.topdown.engine
engine: programs.engine.topdown_engine.TopdownEngine

# should we delete the true data after making DP measurments (1 for True or 0 for False)
delete_raw: 0

[budget]
epsilon_budget_total: 0.0001


#budget in topdown order (e.g. US+PR, State, .... , Block)
geolevel_budget_prop: 0.2,0.2,0.2,0.2,0.2



queriesfile: programs.engine.queries.QueriesCreatorPL94
DPqueries:  race_ethnicity,race, detailed
queriesprop: 0.25, 0.25,  0.5


[constraints]
#the invariants created, (no spaces)
theInvariants.Block: tot,va,gqhh_vect,gqhh_tot
invariants: programs.reader.invariants.InvariantsCreatorPL94

#these are the info to build cenquery.constraint objects
#theConstraints: total,voting_age,hhgq_va_ub,hhgq_va_lb,hhgq_total_lb,hhgq_total_ub,nurse_nva_0
theConstraints.Block: total,hhgq_total_lb,hhgq_total_ub,nurse_nva_0
constraints: programs.reader.constraints.ConstraintsCreatorPL94

minimalSchema: hhgq

[gurobi]
OutputFlag: 1
OptimalityTol: 1e-9
BarConvTol: 1e-8
BarQCPConvTol: 0
BarIterLimit: 1000
FeasibilityTol: 1e-9
Threads: 1
Presolve: -1
NumericFocus: 3

[writer]
#writer: programs.stub_writer.writer
#writer: programs.mdfwriter.MDFWriter
#writer: programs.writer.block_node_writer.BlockNodeWriter
writer: programs.writer.pickled_block_data_writer.PickledBlockDataWriter

keep_attrs: geocode, raw, syn

# Variables Re-used by multiple writers
# Where the data gets written:
output_path: s3://uscb-decennial-ite-das/users/moran331/temp


#Write the Data? 0 or 1
produce_flag: 1

num_parts: 100

[validator]
validator: programs.validator.validator
#validator: programs.stub_validator.validator
results_fname: /mnt/tmp/RA_results

[assessment]

[takedown]
takedown: programs.takedown.takedown
delete_output: True

[experiment]
experiment: programs.experiment.experiment.experiment

run_experiment_flag: 1

experiment_saveloc: s3://uscb-decennial-ite-das/users/moran331/analysis_experiments/RI_test1

# we want to save the = q0,q1original data separate from the privatized data; this allows us to do so
# the original data saveloc only works if the save original data flag is on (1)
save_original_data_flag: 0
original_data_saveloc: s3://uscb-decennial-ite-das/experiments/original_data

# when this is turned on (1), the s3 terminal commands to recursively remove the RDD folders
# will be invoked in order to clear it out before the saveAsPickleFile function gets called
overwrite_flag: 1

filesystem: s3

budget_groups: td1, td2

num_runs: 5

# Budgets follow the order of the geolevels listed in the geodict section
# e.g. Block, Block_Group, Tract, County, State, US+PR

td1.epsilon_budget_total: 4.0
td1.geolevel_budget_prop: 0.2,0.2,0.2,0.2,0.2
td1.DPqueries: detailed
td1.queriesprop: 1.0

td2.epsilon_budget_total: 4.0
td2.geolevel_budget_prop: 0.2,0.2,0.2,0.2,0.2
td2.DPqueries:  race_ethnicity,race,detailed
td2.queriesprop:  0.25, 0.25,0.5


[error_metrics]
error_metrics: programs.metrics.das_error_metrics.error_metrics
