# The 'main' 'testing' file (SFEngine), constantly used. This is the one you should use for "does it run" test

[DEFAULT]
state: ri44
INCLUDE=topdown_sf_USPR.ini

[python]
[gurobi]
[spark]
[logging]
[ENVIRONMENT]
[geodict]
#smallest to largest (no spaces)
geolevel_names: Block,Block_Group,Tract,County,State

#(largest geocode length to smallest, put 0 for US or US+PR (i.e. above state) level)
geolevel_leng: 16,12,11,5,2

[setup]
spark.name: DAS_RI_TEST

[reader]
PersonData.path: s3://uscb-decennial-ite-das/title13_input_data/table8/%(state)s.txt
UnitData.path: s3://uscb-decennial-ite-das/title13_input_data/table8/%(state)s.txt
;reader: programs.reader.pickled_blocks_syn2raw_reader.PickledBlockSyn2RawReader
;pickled.path: s3://uscb-decennial-ite-das/users/zhura301/temp/data
PersonData.class: programs.reader.sql_spar_table.SQLSparseHistogramTable
UnitData.class: programs.reader.sql_spar_table.UnitFromPersonSQLSparseHistogramTable
;PersonData.class: programs.reader.spar_table.SparseHistogramTable

numReaderPartitions: 200
;measure_rdd_times: on
validate_input_data_constraints: off


[engine]
;saved_noisy_app_id: application_1564059690213_0341
;saved_noisy_app_id: application_1560364725809_1968
saved_noisy_app_id: application_1578508239190_0980
;postprocess_only: on
;check_budget: off
;pool_measurements: on
;reload_noisy: off
;save_noisy: off
;noisy_measurements_postfix: noisy_measurements
;spark: off

# should we delete the true data after making DP measurements (1 for True or 0 for False)
delete_raw: 0

[schema]
[budget]
epsilon_budget_total: 1.0
geolevel_budget_prop: 0.2,0.2,0.2,0.2,0.2

[workload]
[constraints]
#the invariants created, (no spaces)
theInvariants.Block: gqhh_vect, gqhh_tot
theInvariants.Tract: tot

#these are the info to build cenquery.constraint objects
theConstraints.Block: hhgq_total_lb, hhgq_total_ub, nurse_nva_0
theConstraints.Tract: total, hhgq_total_lb, hhgq_total_ub

minimalSchema: hhgq

[writer]
#writer: programs.writer.multi_writer.MultiWriter
#multiwriter_writers: BlockNodeDicts, MDFPersonAny
# Where the data gets written:
output_path: $DAS_S3ROOT/users/$JBID/topdown_sf_%(state)s
output_datafile_name: data
produce_flag: 1

# delete existing file (if one) 0 or 1
overwrite_flag: 1
num_parts: 0


[validator]
validator: programs.stub_validator.validator
#validator: programs.stub_validator.validator
results_fname: /mnt/tmp/RA_results

[assessment]

[takedown]
takedown: programs.takedown.takedown
delete_output: False

[experiment]
experiment: programs.experiment.experiment.experiment
run_experiment_flag: 0

[error_metrics]
#error_metrics: programs.metrics.accuracy_metrics_workload.AccuracyMetricsWorkload
#error_metrics: programs.metrics.accuracy_metrics.AccuracyMetrics
