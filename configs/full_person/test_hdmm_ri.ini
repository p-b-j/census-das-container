[DEFAULT]
INCLUDE=../default.ini

[logging]
logfilename: DAS
loglevel: INFO
logfolder: logs

[ENVIRONMENT]
DAS_FRAMEWORK_VERSION: 0.0.1
GRB_ISV_NAME: Census
GRB_APP_NAME: DAS
GRB_Env3: 0
GRB_Env4:

[geodict]
#smallest to largest (no spaces)
#geolevel_names: Block,Block_Group,Tract,Tract_Group,County,State
geolevel_names: Block,Block_SubGroup,Tract,Tract_SubGroup,County,State

#(largest geocode length to smallest, put 0 for US or US+PR (i.e. above state) level)
#geolevel_leng: 16,12,11,9,5,2
geolevel_leng: 16,14,11,8,5,2

[setup]
setup: programs.das_setup.DASDecennialSetup

# Spark config stuff
spark.name: DAS_HDMM_DHCP_RI_TEST
#local[6] tells spark to run locally with 6 threads
#spark.master: local[9]
#Error , only writes to log if there is an error (INFO, DEBUG, ERROR)
spark.master: yarn
spark.loglevel: ERROR

[reader]
#INCLUDE=Reader/unit_simple.ini
#######################################################################################################
# Are these paths correct for how the reader works?
#######################################################################################################


#numReaderPartitions: 5000
#readerPartitionLen: 12
#validate_input_data_constraints: False

reader = programs.reader.table_reader.DASDecennialReader
tables = Person Unit
privacy_table = Person
constraint_tables = Unit
person.class = programs.reader.sql_spar_table.SQLSparseHistogramTable
unit.class = programs.reader.sql_spar_table.SQLSparseHistogramTable
person.path = s3://uscb-decennial-ite-das/title13_input_data/table1a_20190709/ri44.txt
unit.path = s3://uscb-decennial-ite-das/title13_input_data/table10/ri44.txt
delimiter = \t
header = True
person.variables = MAFID age geocode white black aian asian nhopi other hispanic sex citizen relation
unit.variables = MAFID geocode gqtype
linkage = geocode
geocode.type = str
geocode.legal = 0000000000000000-9999999999999999
mafid.type = str
mafid.legal = 000000000-999999999
sex.type = int
sex.legal = 0,1
age.type = int
age.legal = 0-115
hispanic.type = int
hispanic.legal = 0,1
white.type = int
white.legal = 0,1
black.type = int
black.legal = 0,1
aian.type = int
aian.legal = 0,1
asian.type = int
asian.legal = 0,1
nhopi.type = int
nhopi.legal = 0,1
other.type = int
other.legal = 0,1
citizen.type = int
citizen.legal = 0,1
relation.type = int
relation.legal = 0-42
ten.type = int
ten.legal = 0-3
gqtype.type = str
gqtype.legal = 000-999
vacs.type = int
vacs.legal = 0-7
person.recoder = programs.reader.e2e_recoder.DHCP_HHGQ_recoder
person.recode_variables = cenrace hhgq
cenrace = white black aian asian nhopi other
hhgq = relation
cenrace.type = int
cenrace.legal = 0-62
hhgq.type = int
hhgq.legal = 0-7
unit.recoder = programs.reader.hh_recoder.Table10RecoderSimple
unit.recode_variables = hhgqinv
hhgqinv = gqtype
hhgqinv.type = int
hhgqinv.legal = 0-7
person.geography = geocode
person.histogram = hhgq sex age hispanic cenrace citizen
numreaderpartitions = 5000
readerpartitionlen = 12
unit.geography = geocode
unit.histogram = hhgqinv
include = Reader/unit_simple.ini
validate_input_data_constraints = False
root = .
testdir = .
mode = 0


[engine]
engine: programs.engine.hdmm_engine.HDMMEngine

# should we delete the true data after making DP measurements (1 for True or 0 for False)
delete_raw: 0
save_noisy: 0
reload_noisy: 0
check_budget: on

[schema]
schema: REDUCED_DHCP_HHGQ_SCHEMA

[budget]
#######################################################################################################
# Changed the PLB to 1 for this time/test run
#######################################################################################################
epsilon_budget_total: 1


#budget in topdown order (e.g. US, State, .... , Block)
geolevel_budget_prop: 0.2,0.2,0.15,0.15,0.15,0.15




# start with no queries
DPqueries: hhgq, votingage * hispanic * cenrace * citizen, age * sex, detailed
queriesprop: .2, .5, .2,  .1


[constraints]
#the invariants created, (no spaces)
theInvariants.Block: gqhh_vect, gqhh_tot
theInvariants.State: tot

#these are the info to build cenquery.constraint objects
theConstraints.Block: hhgq_total_lb, hhgq_total_ub, hhgq1_lessthan15, hhgq2_greaterthan25, hhgq3_lessthan20, hhgq5_lt16gt65, hhgq6_lt17gt65
theConstraints.State: total, hhgq_total_lb, hhgq_total_ub

minimalSchema: hhgq

[hdmm]
#pidentity or marginal
strategy_type: marginal
#ps_parameters for pidentity strategy type only (needs to be same length as hist_shape)
#ps_parameters: 1,1,2,1

[workload]
#workload: DHCP_HHGQ_Workload_Detailed
workload: DHCP_HHGQ_Workload
#workload: DHCP_PL94_CVAP


[writer]
INCLUDE=Writer/default.ini
#######################################################################################################
# Changed the writer to be the pickled block data writer
#######################################################################################################
writer: programs.writer.pickled_block_data_writer.PickledBlockDataWriter

keep_attrs: geocode, raw, syn
num_parts: 0
# Where the data gets written:
#######################################################################################################
# TODO: Check that the newer writer config options are correct for this New Mexico full_persons test
#######################################################################################################
output_path: $DAS_S3ROOT/users/$JBID/Test_HDMM_RI_pickled_rdd/
output_datafile_name: persons
write_metadata: 0
s3cat: 0
s3cat_suffix: .txt
s3cat_verbose: 1

produce_flag: 1

# delete existing file (if one) 0 or 1
overwrite_flag: 1

[validator]
validator: programs.stub_validator.validator
#validator: programs.stub_validator.validator
results_fname: /mnt/tmp/DK_HDMM_detailed_results

[assessment]

[takedown]
takedown: programs.takedown.takedown
delete_output: 0

[experiment]
experiment: programs.experiment.experiment.experiment
run_experiment_flag: 0

#experiment_saveloc: s3://uscb-decennial-ite-das/users/$JBID/experiments/full_person/HDMM_Experiment/
experiment_saveloc: s3://uscb-decennial-ite-das/users/$JBID/experiments/full_person/HDMM_RI_Experiment/

# we want to save the = q0,q1original data separate from the privatized data; this allows us to do so
# the original data saveloc only works if the save original data flag is on (1)
save_original_data_flag: 0
original_data_saveloc: s3://uscb-decennial-ite-das/experiments/original_data

# when this is turned on (1), the s3 terminal commands to recursively remove the RDD folders
# will be invoked in order to clear it out before the saveAsPickleFile function gets called
overwrite_flag: 1

filesystem: s3

budget_groups: td01, td05, td1, td2, td4

num_runs: 1

# Budgets follow the order of the geolevels listed in the geodict section
# e.g. Block, Block_Group, Tract, County, State

td01.epsilon_budget_total: 0.1
td01.geolevel_budget_prop: 0.2, 0.2, 0.15, 0.15, 0.15, 0.15
td01.DPqueries: hhgq, votingage * hispanic * cenrace * citizen, age * sex,detailed
td01.queriesprop: .2, .5, .2,0.1


td05.epsilon_budget_total: 0.5
td05.geolevel_budget_prop: 0.2, 0.2, 0.15, 0.15, 0.15, 0.15
td05.DPqueries: hhgq, votingage * hispanic * cenrace * citizen, age * sex,detailed
td05.queriesprop: .2, .5, .2,0.1


td1.epsilon_budget_total: 1.0
td1.geolevel_budget_prop: 0.2, 0.2, 0.15, 0.15, 0.15, 0.15
td1.DPqueries: hhgq, votingage * hispanic * cenrace * citizen, age * sex,detailed
td1.queriesprop: .2, .5, .2,0.1


td2.epsilon_budget_total: 2.0
td2.geolevel_budget_prop: 0.2, 0.2, 0.15, 0.15, 0.15, 0.15
td2.DPqueries: hhgq, votingage * hispanic * cenrace * citizen, age * sex,detailed
td2.queriesprop: .2, .5, .2,0.1


td4.epsilon_budget_total: 4.0
td4.geolevel_budget_prop: 0.2, 0.2, 0.15, 0.15, 0.15, 0.15
td4.DPqueries: hhgq, votingage * hispanic * cenrace * citizen, age * sex,detailed
td4.queriesprop: .2, .5, .2,0.1





[error_metrics]
error_metrics: programs.metrics.accuracy_metrics.AccuracyMetrics
