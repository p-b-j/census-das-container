# Sample config file for running DAS in the container
# Based heavily on das_decennial/configs/run.ini
# and das_decennial/configs/production/pl94

[logging]
logfilename = DAS
loglevel = INFO
logfolder = logs

[ENVIRONMENT]
das_framework_version = 1.1.0
GRB_ISV_NAME = Standalone
cluster = EDU
GRB_APP_NAME = DAS
GRB_Env3 = 0
GRB_Env4 =

[geodict]:
aian_areas = Legal_Federally_Recognized_American_Indian_Area,
    American_Indian_Joint_Use_Area,
    Hawaiian_Home_Land,
    Alaska_Native_Village_Statistical_Area,
    State_Recognized_Legal_American_Indian_Area,
    Oklahoma_Tribal_Statistical_Area,
    Joint_Use_Oklahoma_Tribal_Statistical_Area
geocode_length = 16
geolevel_leng = 16,12,11,5,2,0
geolevel_names = Block,Block_Group,Tract,County,State,US
geo_bottomlevel = Block
geo_path = 
geo_toplevel = 
ignore_gqs_in_block_groups = False
target_das_aian_areas = True
spine = opt_spine

[setup]
setup = programs.das_setup.DASDecennialSetup
spark.name = DAS_CEF_TEST
#Error , only writes to log if there is an error (INFO, DEBUG, )
spark.loglevel = ERROR

[reader]
tables = Unit Person
constraint_tables = Unit
privacy_table = Person

reader = programs.reader.table_reader.DASDecennialReader
readerpartitionlen = 14
delimiter = |
grfc_path = $HOME/das_files/grfc_combined.csv
header = False
numreaderpartitions = 20

person.class = programs.reader.cef_2020.cef_2020_dhcp_reader.CEF2020PersonsTable
person.generated_module = programs.reader.cef_2020.cef_validator_classes
person.generated_table = CEF20_PER
person.geography = geocode
Person.histogram = relgq sex age hispanic cenrace_das
person.path = $HOME/das_files/converted_synth_pop.cef
Person.recoder = programs.reader.cef_2020.cef_2020_dhcp_reader.DHCP_recoder
Person.recode_variables = relgq sex age hispanic cenrace_das

unit.class = programs.reader.cef_2020.cef_2020_dhcp_reader.CEF2020DHCPUnitTable
unit.generated_module = programs.reader.cef_2020.cef_validator_classes
unit.generated_table = CEF20_UNIT
unit.geography = geocode
Unit.histogram = hhgq_unit_dhcp
unit.path = $HOME/das_files/converted_synth_unit.cef
Unit.recoder = programs.reader.hh_recoder.Table10RecoderDHCP
Unit.recode_variables = hhgq_unit_dhcp
validate_input_data_constraints = True

relgq = relship qgqtyp
relgq.type = int
relgq.legal = 0-41

sex = qsex
sex.type = int
sex.legal = 0-1

age = qage
age.type = int
age.legal = 0-115

hispanic = cenhisp
hispanic.type = int
hispanic.legal = 0-1

cenrace_das = cenrace
cenrace_das.type = int
cenrace_das.legal = 0-62

hhgq_unit_dhcp = qgqtyp
hhgq_unit_dhcp.type = int
hhgq_unit_dhcp.legal = 0-24


[engine]
check_budget = off
delete_raw = 0
engine = programs.engine.topdown_engine.TopdownEngine
noisy_measurements_postfix = NMF10_PER_US
reload_noisy = 0
save_noisy = TRUE

[schema]
schema = DHCP_SCHEMA

[budget]
approx_dp_delta = 1e-10
dp_mechanism = discrete_gaussian_mechanism
epsilon_budget_total= 4
geolevel_budget_prop = 0.18,0.18,0.18,0.18,0.18,0.1
only_dyadic_rationals = False
privacy_framework = zcdp

query_ordering = test_dhcp_ordering
strategy = test_dhcp_strategy
; query_ordering = Strategy1b_ST_CTY_TR_BG_isoTot_Ordering_dsepJune3
; strategy = ProductionCandidate20210527US_mult8_add02_dsepJune3
global_scale = 339/542

; DPqueries = total, relgq, votingage * hispanic * cenrace, age * sex * hispanic * cenrace, detailed
; queriesprop = 0.1, 0.2, 0.4, 0.2, 0.1

; L2_DPqueryPart0= total, relgq
; L2_DPqueryPart1= votingage * hispanic * cenrace
; L2_DPqueryPart2= age * sex * hispanic * cenrace
; L2_DPqueryPart3= detailed


; [workload]
; workload: old_pl94_manual_workload

[constraints]
theInvariants.Block = gqhh_vect, gqhh_tot
theInvariants.State = tot
theConstraints.Block = hhgq_total_lb, hhgq_total_ub, householder_ub, relgq0_lt15, relgq1_lt15, relgq2_lt15, relgq3_lt15, relgq4_lt15, relgq5_lt15, relgq6_gt89, relgq7_gt89, relgq8_gt89, relgq10_lt30, relgq11_gt74, relgq12_lt30, relgq13_lt15_gt89, relgq16_gt20, relgq18_lt15, relgq19_lt15, relgq20_lt15, relgq21_lt15, relgq22_lt15, relgq23_lt17_gt65, relgq24_gt25, relgq25_gt25, relgq26_gt25, relgq27_lt20, relgq31_lt17_gt65, relgq32_lt3_gt30, relgq33_lt16_gt65, relgq34_lt17_gt65, relgq35_lt17_gt65, relgq37_lt16, relgq38_lt16, relgq39_lt16_gt75, spousesUnmarriedPartners_ub, people100Plus_ub, parents_ub, parentInLaws_ub
# US and State constraints are different for US and PR, and are defined explicitly in person_US.ini and person_PR.ini
# Explicitly setting the State and US constraints here because they differ from PR
theConstraints.State = total, hhgq_total_lb, hhgq_total_ub
minimalSchema = relgq

[writer]
certificate_name = A very precise data set
certificate_person1 = Ben Bitdiddle
certificate_person2 = Alyssa P. Hacker
certificate_suffix = .certificate.pdf
certificate_title = Certificate of Disclosure Avoidance
certificate_title1 = Novice Programmer
certificate_title2 = Supervisor
drb_clearance_number = CBDRB-FY21-DSEP-005
classification_level = C_U_I//SP-CENS - Title 13 protected data

keep_attrs = geocode, syn, unit_syn, _invar, _cons, raw, raw_housing
multiwriter_writers = BlockNodeDicts, DHCP2020_MDF
num_parts = 10
output_datafile_name = MDF10_PER_US

stats_dir = $HOME/das_files/upload
output_path = $HOME/das_files/results

upload_logfile = 0
writer = programs.writer.multi_writer.MultiWriter
write_metadata = 1
overwrite_flag = 1
produce_flag = 1
s3cat = 1
s3cat_suffix = .txt
s3cat_verbose = 0
save_git_commit = 1

[validator]
validator = programs.stub_validator.validator

[takedown]
takedown = programs.takedown.takedown
delete_output = 0

[experiment]
run_experiment_flag: 0

[error_metrics]
population_cutoff = 500

error_metrics = programs.metrics.accuracy_metrics.AbstractDASErrorMetrics
calculate_binned_query_errors = True
calculate_per_query_quantile_errors = True
calculate_per_query_quantile_signed_errors = True
l1_relative_error_geolevels = Place, Block_Group, OSE
l1_relative_error_queries = cenrace_7lev_two_comb * hispanic, gqlevels
print_blau_quintile_errors = True
print_8_cell_cenrace_hisp_errors = True
print_place_mcd_ose_bg_l1_error_on_total_pop = True
print_aians_l1_error_on_total_pop = True

[gurobi]
gurobi_path = /usr/local/gurobi911/linux64/${PYTHON_VERSION}_utf32/
gurobi_logfile_name = gurobi.log
gurobi_lic = ${HOME}/gurobi.lic
barconvtol = 0.0
bariterlimit = 1000
dataindnpass_tolerancetype = opt_tol
feasibilitytol = 1e-7
l2_acceptable_statuses = OPTIMAL, SUBOPTIMAL, ITERATION_LIMIT
l2_grb_algorithm = -1
l2_grb_presolve = -1
l2_grb_presparsify = -1
l2_optimization_approach = DataIndUserSpecifiedQueriesNPass
l2_suboptimal_allowed = False
method = -1
numericfocus = 3
optimalitytol = 1e-6
opt_tol_slack = 0.1
outputflag = 1
presolve = -1
python_presolve = 1
rounder_acceptable_statuses = OPTIMAL
rounder_optimization_approach = MultipassRounder
seq_optimization_approach = L2PlusRounderWithBackup_interleaved
record_gurobi_stats = True
record_cpu_stats: False
print_gurobi_stats: True
gurobi_lic_create: False

# Control the number of threads used by Gurobi
# threads = 96
# Threads for the top-geolevel
# threads_root2root = 96
# Threads for each geolevel (if not top geolevel)
# threads_state = 96
# threads_county = 32
# Not used, commenting out
# threads_tract_group = 96
# threads_tract = 16
# threads_block_group = 8
# threads_block = 4

[stats]
heartbeat_frequency = 0
# Notify when GC is run on the master node
notify_gc_master = 0

[monitoring]
# Heartbeat just tells the user and the dashboard that we are alive
print_heartbeat = False
print_heartbeat_frequency = 0
send_stacktrace = False
heartbeat_frequency = 0

# Do we log to the dashboard on successful token acquisitions and retries?
# notifying that we got a token is just for debugging; this will typically be false
notify_dashboard_gurobi_success = false

# notifying that we had to retry is a problem; typically this will be true
notify_dashboard_gurobi_retry = false

# Notifications are about the current execution of the optimizer.
# It's collected from syslog.
notification_frequency = 0

[alert]
# Print this message when the system starts up
message = Hello world!
