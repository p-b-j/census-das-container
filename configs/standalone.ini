[DEFAULT]
# root specifies the root location for all files; testdir specifies ???; mode specifies ???
# For the demo, the root in the current directory
root: .
testdir: .
mode: 0
INCLUDE=../das_decennial/configs/default.ini

[spark]
# Whatever spark options you may have

[logging]
logfilename: DAS
loglevel: INFO
logfolder: logs

[ENVIRONMENT]
DAS_FRAMEWORK_VERSION: 0.0.1
GRB_ISV_NAME: Standalone
cluster: GAM
GRB_APP_NAME: DAS
GRB_Env3: 0
GRB_Env4:

[geodict]:
geolevel_names: Enumdist,Supdist,County,State,National
geolevel_leng: 13,9,6,2,0

[setup]
setup: programs.das_setup.DASDecennialSetup

# Spark config stuff
spark.name: DAS_1940
#local[6] tells spark to run locally with 6 threads
#spark.master: local[9]
#Error , only writes to log if there is an error (INFO, DEBUG, )
spark.loglevel: Error

[reader]
delimiter: \t
header: True

PersonData.generated_module: programs.reader.ipums_1940.ipums_1940_classes
PersonData.generated_table: P

UnitData.generated_module: programs.reader.ipums_1940.ipums_1940_classes
UnitData.generated_table: H

;PersonData.path= s3://uscb-decennial-ite-das/1940/min_EXT.dat
;UnitData.path= s3://uscb-decennial-ite-das/1940/min_EXT.dat

PersonData.geography: geocode
PersonData.histogram: hhgq1940 sex1940 age1940 hispanic1940 cenrace1940 citizen1940

UnitData.geography= geocode
UnitData.histogram= hhgq1940

PersonData.recoder= programs.reader.ipums_1940.ipums_1940_reader.person_recoder
PersonData.recode_variables= hhgq1940 sex1940 age1940 hispanic1940 cenrace1940 citizen1940
hhgq1940= gq gqtype
hhgq1940.type= int
hhgq1940.legal= 0-7
sex1940= sex
sex1940.type= int
sex1940.legal= 0,1
age1940= age
age1940.type= int
age1940.legal= 0-115
hispanic1940= hispan
hispanic1940.type= int
hispanic1940.legal= 0,1
cenrace1940= race
cenrace1940.type= int
cenrace1940.legal= 0-5
citizen1940= citizen
citizen1940.type= int
citizen1940.legal= 0,1

UnitData.recoder= programs.reader.ipums_1940.ipums_1940_reader.unit_recoder
UnitData.recode_variables= hhgq1940 geocode
geocode= statefip county supdist enumdist
geocode.type= str
geocode.legal= 0000000000000-9999999999999

PersonData.path: $HOME/das_files/EXT1940USCB.dat
UnitData.path: $HOME/das_files/EXT1940USCB.dat

reader: programs.reader.table_reader.DASDecennialReader

tables: UnitData PersonData

privacy_table: PersonData
constraint_tables: UnitData

PersonData.class: programs.reader.ipums_1940.ipums_1940_reader.Persons1940Table
UnitData.class: programs.reader.ipums_1940.ipums_1940_reader.Household1940Table
validate_input_data_constraints: false
;numReaderPartitions: 50

[engine]
engine: programs.engine.topdown_engine.TopdownEngine
check_budget: off
;pool_measurements: on

# should we delete the true data after making DP measurments (1 for True or 0 for False)
delete_raw: 0

[schema]
schema: 1940

[budget]
strategy: test_strategy
query_ordering: test_strategy_regular_ordering
#budget in topdown order (e.g. US, State, .... , Block)
epsilon_budget_total = 1.0

#budget in topdown order (e.g. US, State, .... , Block)
geolevel_budget_prop = 0.2, 0.2, 0.2, 0.2, 0.2

global_scale: 1/1



# DP queries to create, (or None) (total budget proporiton must add to 1.0)
# DPqueries: hhgq, votingage * race, hispanic * citizen, detailed
# queriesprop: 0.25, 0.25, 0.25,  0.25

[workload]
workload: WORKLOAD_1940

[constraints]
theInvariants.Enumdist: gqhh_vect,gqhh_tot
theInvariants.State: tot

theConstraints.Enumdist: hhgq_total_lb,hhgq_total_ub
theConstraints.State: total,hhgq_total_lb,hhgq_total_ub

minimalSchema: hhgq1940


[writer]
writer: programs.writer.ipums_1940.ipums_1940_writer.IPUMSPersonWriter
stats_dir: $HOME/das_files/upload
output_path: $HOME/das_files/results
output_datafile_name: data

#Write the Data? 0 or 1
produce_flag: 1
#keep_attrs: geocode, syn, raw_housing

#options for block_node_write 
# delete existing file (if one) 0 or 1
overwrite_flag: 1

s3cat: 1
s3cat_suffix: .txt
s3cat_verbose: 1
num_parts: 0

[validator]
validator: programs.stub_validator.validator
#validator: programs.stub_validator.validator
results_fname: $HOME/das_files/valid_results

[assessment]

[takedown]
takedown: programs.takedown.takedown
delete_output: False

[experiment]
run_experiment_flag: 0

[error_metrics]
error_metrics: programs.metrics.accuracy_metrics_workload.AccuracyMetricsWorkload

[gurobi]
gurobi_lic: ${HOME}/gurobi.lic
record_gurobi_stats: False
record_cpu_stats: False
print_gurobi_stats: True
